{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ub1i-2Kz5rPv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "outputId": "1ba4875f-47e3-4654-91f6-0bf509bb6dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: Keras 2.3.1\n",
            "Uninstalling Keras-2.3.1:\n",
            "  Would remove:\n",
            "    /tensorflow-1.15.2/python3.7/Keras-2.3.1.dist-info/*\n",
            "    /tensorflow-1.15.2/python3.7/docs/*\n",
            "    /tensorflow-1.15.2/python3.7/keras/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled Keras-2.3.1\n",
            "Requirement already satisfied: keras==2.1.5 in /usr/local/lib/python3.7/dist-packages (2.1.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (1.21.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (1.16.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (1.4.1)\n",
            "Collecting h5py==2.10.0\n",
            "  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "Collecting six\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting numpy>=1.7\n",
            "  Using cached numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Installing collected packages: six, numpy, h5py\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lucid 0.3.10 requires umap-learn, which is not installed.\n",
            "tensorflow 1.15.2 requires gast==0.2.2, but you have gast 0.5.3 which is incompatible.\n",
            "lucid 0.3.10 requires numpy<=1.19, but you have numpy 1.21.6 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed h5py-2.10.0 numpy-1.21.6 six-1.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py",
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip uninstall keras\n",
        "!pip install keras==2.1.5 \n",
        "#!pip install tensorflow-gpu==1.6.0\n",
        "!pip install 'h5py==2.10.0' --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U3UzqmVL5lE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ccdd2bd-d636-4974-e297-c4dc0224b981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt \n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wutCcb-XPM2h",
        "outputId": "38a2d913-b5e8-4e72-e443-50a02d8ce67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.2\n",
            "2.1.5\n",
            "2.10.0\n"
          ]
        }
      ],
      "source": [
        "#IF THE VERSION ARE NOT CORRECT RESTART RUNTIME. \n",
        "#Try also restart run, reinstall\n",
        "# correct versions below\n",
        "print(tf.__version__) #1.15.2\n",
        "print(keras.__version__)  #2.1.5\n",
        "print(h5py.__version__) # 2.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At51lj1gdb29"
      },
      "outputs": [],
      "source": [
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
        "#session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkin8pGk6GCx",
        "outputId": "d974250d-9c3e-41b2-e444-69832c990c79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'keras-yolo3'...\n",
            "remote: Enumerating objects: 144, done.\u001b[K\n",
            "remote: Total 144 (delta 0), reused 0 (delta 0), pack-reused 144\u001b[K\n",
            "Receiving objects: 100% (144/144), 151.08 KiB | 2.29 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n"
          ]
        }
      ],
      "source": [
        "#The repo below is a popular repo that is mentioned repeatable for those who want to translate darknet models to keras models. Primarly yolov3\n",
        "!git clone https://github.com/qqwweee/keras-yolo3 \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB6ODMdc9pwC",
        "outputId": "5ec91593-c8c9-452a-c921-5c96383e45d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/keras-yolo3\n"
          ]
        }
      ],
      "source": [
        "%cd keras-yolo3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbS0TYryGK_8"
      },
      "outputs": [],
      "source": [
        "#IMPORTING all weights and PASCAL training set. The above repo included a data preprocessing script that is used \"voc_annotation.py\"\n",
        "#The pretrained weights are retrieved form here and the dataset\n",
        "#!wget https://pjreddie.com/media/files/yolov3.weights\n",
        "#!wget https://pjreddie.com/media/files/yolov3-tiny.weights\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1S1V-V0JtbQE6Z0LQmEMNPJJyJYgjFSsY' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1S1V-V0JtbQE6Z0LQmEMNPJJyJYgjFSsY\" -O fine_tune_yolov3.h5 && rm -rf /tmp/cookies.txt\n",
        "\n",
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
        "!tar xvf VOCtrainval_06-Nov-2007.tar\n",
        "!tar xvf VOCtest_06-Nov-2007.tar\n",
        "!python voc_annotation.py \n",
        "#USING REPO convert.py to convert from darknet model to keras model and save it a uploadable .h5 file\n",
        "# !python  convert.py yolov3.cfg yolov3.weights model_data/yolov3_weights.h5\n",
        "# !python convert.py yolov3-tiny.cfg yolov3-tiny.weights model_data/yolov3-tiny_weights.h5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from train import *\n",
        "from yolo import *\n"
      ],
      "metadata": {
        "id": "qdJLmOTXX4UY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing data parameters\n",
        "input_shape = (416,416)\n",
        "annotation_path_train = '2007_test.txt'\n",
        "annotation_path_test = '2007_test.txt'\n",
        "log_dir = 'logs/000/'\n",
        "classes_path = 'model_data/voc_classes.txt'\n",
        "anchors_path = 'model_data/yolo_anchors.txt'\n",
        "class_names = get_classes(classes_path)\n",
        "num_classes = len(class_names)\n",
        "anchors = get_anchors(anchors_path)\n",
        "\n",
        "val_split = 0.1\n",
        "training_lines = None\n",
        "with open(annotation_path_train) as f:\n",
        "  training_lines = f.readlines()\n",
        "\n",
        "np.random.seed(10101)\n",
        "np.random.shuffle(training_lines)\n",
        "np.random.seed(None)\n",
        "num_val = int(len(training_lines)*val_split)\n",
        "num_train = len(training_lines) - num_val\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "#Pre-trained weights\n",
        "path_teacher = \"/content/keras-yolo3/model_data/yolov3_weights.h5\"\n",
        "path_student = \"/content/keras-yolo3/model_data/yolov3-tiny_weights.h5\"\n",
        "\n",
        "#Fine-tuned_weights\n",
        "path_teacher_fine_tuned = \"/content/keras-yolo3/fine_tune_yolov3.h5\"\n",
        "\n",
        "#TODO FIX A STUDENT MODEL\n",
        "# anchors_tiny_path = 'model_data/tiny_yolo_anchors.txt'\n",
        "# anchors_tiny = get_anchors(anchors_tiny_path)"
      ],
      "metadata": {
        "id": "I1oe4beVYIMe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pre_trained_model(input_shape, anchors, num_classes, weights_path):\n",
        "  #TODO INTRODUCE CONVERTERT\n",
        "  #based on repo keras-yolo3 train.py  .\n",
        "  #This allows to create models from the previously created .h5 file and train them.\n",
        "\n",
        "  model = create_model(input_shape, anchors, num_classes, weights_path)\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "prksYoOdY7FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pre_trained_tiny_model(input_shape, anchors, num_classes, weight_path):\n",
        "  #TODO INTRODUCE CONVERTER\n",
        "  #based on repo keras-yolo3 train.py  .\n",
        "  #This allows to create models from the previously created .h5 file and train them.\n",
        "\n",
        "  model = create_tiny_model(input_shape, anchors, num_classes, weights_path)\n",
        "  return model"
      ],
      "metadata": {
        "id": "SfjG7MdEbY6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_fine_tuned_model(weight_path, anchors_path, classes_path):\n",
        "# Must change in generate method:\n",
        "# load_model add arg: by_name = True\n",
        "\n",
        "  defaults = {\n",
        "        \"model_path\": weight_path,\n",
        "        \"anchors_path\": anchors_path,\n",
        "        \"classes_path\": classes_path,\n",
        "        \"score\" : 0.3,\n",
        "        \"iou\" : 0.45,\n",
        "        \"model_image_size\" : (416, 416),\n",
        "        \"gpu_num\" : 1,\n",
        "    }\n",
        "  model = YOLO(**defaults)\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "kOI1CzQdbppB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_fine_tuned_model(path_teacher_fine_tuned, anchors_path, classes_path)"
      ],
      "metadata": {
        "id": "J8Wd3zUNdnjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, training_lines, num_train, batch_size, input_shape, anchors, num_classes):\n",
        "  #based on train.py in repo\n",
        "  lines = training_lines\n",
        "\n",
        "  logging = TensorBoard(log_dir=log_dir)\n",
        "  checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
        "      monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
        "                                 \n",
        "\n",
        "  for i in range(len(model.layers)):\n",
        "    model.layers[i].trainable = True\n",
        "  \n",
        "  model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
        "\n",
        "\n",
        "  print('Unfreeze all of the layers.')\n",
        "\n",
        "  model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
        "    steps_per_epoch=max(1, num_train//batch_size),\n",
        "    validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
        "    validation_steps=max(1, num_val//batch_size),\n",
        "    epochs=150,  \n",
        "    initial_epoch=100,\n",
        "    callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "iT1IhxuVZoCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_generator(annotation_path, batch_size, input_shape, anchors, num_classes):\n",
        "  #based on data_generator in train.py\n",
        "  lines = None\n",
        " \n",
        "  with open(annotation_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "  n = len(lines)\n",
        "  i = 0\n",
        "  while True:\n",
        "    image_data = []\n",
        "    box_data = []\n",
        "    for i in range(batch_size):\n",
        "      image, box = get_random_data(lines[i], input_shape, random=False)\n",
        "      image_data.append(image)\n",
        "      box_data.append(box)\n",
        "      i = (i+1) % n\n",
        "    image_data = np.array(image_data)\n",
        "    real_image = []\n",
        "    for image in image_data:\n",
        "      real_image.append(Image.fromarray(np.uint8(image*255)))\n",
        "    box_data = np.array(box_data)\n",
        "    y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
        "\n",
        "    yield real_image, [*y_true]\n"
      ],
      "metadata": {
        "id": "Qv_HTtvRRR16"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = test_generator(annotation_path_test, batch_size, input_shape, anchors, num_classes)"
      ],
      "metadata": {
        "id": "kpCzDqtkNdTO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, y_trues = next(generator)"
      ],
      "metadata": {
        "id": "fO3mQ-1YkR30"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7D7pVBdSe4H"
      },
      "outputs": [],
      "source": [
        "# These classes and methods are based on https://keras.io/examples/vision/knowledge_distillation/\n",
        "\n",
        "class Distiller(tf.keras.Model):\n",
        "  def __init__(self, student, teacher):\n",
        "    super(Distiller, self).__init__()\n",
        "    self.teacher = teacher\n",
        "    self.student = student\n",
        "\n",
        "  def compile(sef, optimizer, metrices\n",
        "              student_loss, distillation_loss, alpha=0.1, temperature=3):\n",
        "    \n",
        "     super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss = student_loss\n",
        "        self.distillation_loss = distillation_loss\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "  \n",
        "  def train(self, data):\n",
        "\n",
        "    x,y = data\n",
        "    teacher_predictions = self.teacher(x, training=False)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "            # Forward pass of student\n",
        "            student_predictions = self.student(x, training=True)\n",
        "           \n",
        "            student_loss = self.student_loss(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            \n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update the metrics configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "  def eval(self, data):\n",
        "\n",
        "    x,y = data\n",
        "\n",
        "    y_predictions = self.student(x, training=False)\n",
        "\n",
        "    student_loss = self.student_loss(y, y_prediction)\n",
        "\n",
        "    self.compiled_metrics.update_state(y, y_prediction)\n",
        "\n",
        "    results = {m.name: me.result() for m in self.metrics}\n",
        "    results.update({\"student_loss\": student_loss})\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teacher = create_fine_tuned_model()\n",
        "student = create_tiny_pre_trained_model()\n",
        "kd = Distiller(teacher, student)\n",
        "kd.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred})\n",
        "kd.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "Rd1AsviU0pJ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Project yolov3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}